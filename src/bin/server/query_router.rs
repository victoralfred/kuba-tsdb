//! Query Router - SQL/PromQL Parsing and Execution
//!
//! This module bridges the gap between the HTTP API and the query engine.
//! It parses SQL/PromQL queries, extracts parameters, and uses TimeSeriesDB
//! methods to retrieve and aggregate data.
//!
//! # Query Result Caching
//!
//! The router integrates with `QueryCache` to cache parsed query results:
//! - SELECT and AGGREGATE queries are cached based on AST hash
//! - Cache hits return results immediately without database access
//! - Cache misses execute the query and populate the cache

use super::aggregation::{
    aggregate_into_buckets, calculate_auto_interval, compute_aggregation_from_ast,
    create_time_aggregation_info, format_interval_ms, validate_interval, DEFAULT_TARGET_POINTS,
};
use super::types::{
    AggregateExecutionResult, CostEstimateInfo, ExecutionStep, ExplainResult,
    GroupedAggregationResult, OptimizationInfo, SeriesData,
};
use kuba_tsdb::cache::SharedQueryCache;
use kuba_tsdb::engine::TimeSeriesDB;
use kuba_tsdb::query::ast::{
    AggregateQuery, DownsampleMethod, DownsampleQuery, LatestQuery, SelectQuery,
};
use kuba_tsdb::query::result::{QueryResult, ResultData, ResultRow, SeriesResult};
use kuba_tsdb::query::{
    parse_promql, parse_sql, AggregationFunction as QueryAggFunction, Query as ParsedQuery,
};
use kuba_tsdb::types::{DataPoint, SeriesId};
use std::collections::HashMap;

/// Detected query language
#[derive(Debug, Clone, Copy)]
pub enum QueryLanguage {
    Sql,
    PromQL,
}

impl std::fmt::Display for QueryLanguage {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            QueryLanguage::Sql => write!(f, "sql"),
            QueryLanguage::PromQL => write!(f, "promql"),
        }
    }
}

/// Query execution result containing all possible output types
pub type QueryExecutionResult = (
    QueryLanguage,
    String,
    Vec<DataPoint>,
    Option<AggregateExecutionResult>,
    Option<Vec<SeriesData>>,
    Option<ExplainResult>,
);

/// Execute a query string against the database with optional caching
///
/// Auto-detects the query language and routes to appropriate parser.
/// When a cache is provided, SELECT queries will check the cache first
/// and populate it on cache misses.
///
/// # Arguments
/// * `db` - The TimeSeriesDB instance
/// * `query_str` - The SQL or PromQL query string
/// * `language` - Query language hint ("sql", "promql", or "auto")
/// * `cache` - Optional query cache for result caching
///
/// # Returns
/// A tuple containing:
/// - `QueryLanguage`: Detected query language
/// - `String`: Query type (select, aggregate, etc.)
/// - `Vec<DataPoint>`: Raw data points (for backward compatibility)
/// - `Option<AggregateExecutionResult>`: Full aggregation result with time windowing
/// - `Option<Vec<SeriesData>>`: Series-grouped data for SELECT queries
/// - `Option<ExplainResult>`: EXPLAIN result with execution plan (for EXPLAIN queries)
#[allow(dead_code)] // Kept for backward compatibility / non-caching use cases
pub async fn execute_query(
    db: &TimeSeriesDB,
    query_str: &str,
    language: &str,
) -> Result<QueryExecutionResult, String> {
    execute_query_with_cache(db, query_str, language, None).await
}

/// Execute a query with optional result caching
///
/// This is the core execution function that supports caching.
/// SELECT queries are cached based on their AST hash.
pub async fn execute_query_with_cache(
    db: &TimeSeriesDB,
    query_str: &str,
    language: &str,
    cache: Option<&SharedQueryCache>,
) -> Result<QueryExecutionResult, String> {
    // Detect and parse query
    let (parsed, lang) = match language.to_lowercase().as_str() {
        "sql" => {
            let q = parse_sql(query_str).map_err(|e| format!("SQL parse error: {}", e))?;
            (q, QueryLanguage::Sql)
        },
        "promql" => {
            let q = parse_promql(query_str).map_err(|e| format!("PromQL parse error: {}", e))?;
            (q, QueryLanguage::PromQL)
        },
        _ => {
            // Auto-detect based on query content
            let trimmed = query_str.trim().to_uppercase();
            if trimmed.starts_with("SELECT") || trimmed.starts_with("EXPLAIN") {
                let q = parse_sql(query_str).map_err(|e| format!("SQL parse error: {}", e))?;
                (q, QueryLanguage::Sql)
            } else {
                let q =
                    parse_promql(query_str).map_err(|e| format!("PromQL parse error: {}", e))?;
                (q, QueryLanguage::PromQL)
            }
        },
    };

    // Check cache for SELECT queries
    if let Some(query_cache) = cache {
        if let Some(cached_result) = query_cache.get(&parsed) {
            tracing::debug!("Query cache hit for {:?}", query_str);
            return convert_cached_result(cached_result, lang);
        }
    }

    // Execute based on query type
    let result = match &parsed {
        ParsedQuery::Select(q) => {
            let (lang, qtype, points, series_data) = execute_select(db, q.clone(), lang).await?;

            // Cache the result for SELECT queries
            if let (Some(query_cache), Some(ref sd)) = (cache, &series_data) {
                let query_result = convert_to_query_result(sd);
                query_cache.put(&parsed, query_result);
                tracing::debug!("Cached SELECT query result");
            }

            Ok((lang, qtype, points, None, series_data, None))
        },
        ParsedQuery::Aggregate(q) => {
            let (lang, qtype, points, agg_result) = execute_aggregate(db, q.clone(), lang).await?;

            // Cache aggregate results (they're expensive to compute)
            if let Some(query_cache) = cache {
                let query_result =
                    QueryResult::from_rows(points.iter().map(|p| ResultRow::from(*p)).collect());
                query_cache.put(&parsed, query_result);
                tracing::debug!("Cached AGGREGATE query result");
            }

            Ok((lang, qtype, points, Some(agg_result), None, None))
        },
        ParsedQuery::Downsample(q) => {
            let (lang, qtype, points) = execute_downsample(db, q.clone(), lang).await?;

            // Cache downsample results
            if let Some(query_cache) = cache {
                let query_result = QueryResult::from_points(points.clone());
                query_cache.put(&parsed, query_result);
                tracing::debug!("Cached DOWNSAMPLE query result");
            }

            Ok((lang, qtype, points, None, None, None))
        },
        ParsedQuery::Latest(q) => {
            let (lang, qtype, points) = execute_latest(db, q.clone(), lang).await?;

            // Cache latest results (short TTL would be ideal, but using default)
            if let Some(query_cache) = cache {
                let query_result = QueryResult::from_points(points.clone());
                query_cache.put(&parsed, query_result);
                tracing::debug!("Cached LATEST query result");
            }

            Ok((lang, qtype, points, None, None, None))
        },
        ParsedQuery::Explain(inner) => {
            let explain_result = generate_explain_plan(inner);
            Ok((
                lang,
                "explain".to_string(),
                vec![],
                None,
                None,
                Some(explain_result),
            ))
        },
        ParsedQuery::Stream(_) => Err(
            "Stream queries are not supported via HTTP. Use WebSocket for streaming.".to_string(),
        ),
    };

    result
}

/// Convert SeriesData to QueryResult for caching
fn convert_to_query_result(series_data: &[SeriesData]) -> QueryResult {
    let series: Vec<SeriesResult> = series_data
        .iter()
        .enumerate()
        .map(|(idx, sd)| {
            let values: Vec<(i64, f64)> =
                sd.pointlist.iter().map(|p| (p[0] as i64, p[1])).collect();

            // Parse tags from tag_set (format: "key:value")
            let tags: HashMap<String, String> = sd
                .tag_set
                .iter()
                .filter_map(|t| {
                    let parts: Vec<&str> = t.splitn(2, ':').collect();
                    if parts.len() == 2 {
                        Some((parts[0].to_string(), parts[1].to_string()))
                    } else {
                        None
                    }
                })
                .collect();

            // Store metric name in tags for reconstruction
            let mut tags_with_metric = tags;
            tags_with_metric.insert("__metric__".to_string(), sd.metric.clone());

            SeriesResult::new(idx as u128)
                .with_tags(tags_with_metric)
                .with_values(values)
        })
        .collect();

    QueryResult::from_series(series)
}

/// Convert cached QueryResult back to the execute_query return format
fn convert_cached_result(
    result: QueryResult,
    lang: QueryLanguage,
) -> Result<QueryExecutionResult, String> {
    match result.data {
        ResultData::Series(series) => {
            let series_data: Vec<SeriesData> = series
                .into_iter()
                .map(|s| {
                    let pointlist: Vec<[f64; 2]> = s
                        .values
                        .iter()
                        .map(|(ts, val)| [*ts as f64, *val])
                        .collect();
                    let length = pointlist.len();

                    // Extract metric name from special tag
                    let metric = s
                        .tags
                        .get("__metric__")
                        .cloned()
                        .unwrap_or_else(|| "unknown".to_string());

                    // Build tag_set excluding internal __metric__ tag
                    let tag_set: Vec<String> = s
                        .tags
                        .iter()
                        .filter(|(k, _)| *k != "__metric__")
                        .map(|(k, v)| format!("{}:{}", k, v))
                        .collect();
                    let scope = tag_set.join(",");

                    SeriesData {
                        metric,
                        scope,
                        tag_set,
                        pointlist,
                        length,
                    }
                })
                .collect();

            // Reconstruct DataPoints for backward compatibility
            let points: Vec<DataPoint> = series_data
                .iter()
                .flat_map(|sd| {
                    sd.pointlist
                        .iter()
                        .map(|p| DataPoint::new(0, p[0] as i64, p[1]))
                })
                .collect();

            Ok((
                lang,
                "select".to_string(),
                points,
                None,
                Some(series_data),
                None,
            ))
        },
        ResultData::Scalar(val) => Ok((
            lang,
            "scalar".to_string(),
            vec![DataPoint::new(0, 0, val)],
            None,
            None,
            None,
        )),
        _ => Ok((lang, "unknown".to_string(), vec![], None, None, None)),
    }
}

/// Execute a SELECT query
async fn execute_select(
    db: &TimeSeriesDB,
    q: SelectQuery,
    lang: QueryLanguage,
) -> Result<
    (
        QueryLanguage,
        String,
        Vec<DataPoint>,
        Option<Vec<SeriesData>>,
    ),
    String,
> {
    // Get series IDs from selector
    let series_ids: Vec<SeriesId> = match q.selector.series_id {
        Some(id) => vec![id],
        None => {
            if let Some(ref measurement) = q.selector.measurement {
                let tag_filter = q.selector.to_tag_filter();
                db.find_series(measurement, &tag_filter)
                    .await
                    .map_err(|e| format!("Series lookup error: {}", e))?
            } else {
                return Err("No series_id or measurement specified in query".to_string());
            }
        },
    };

    if series_ids.is_empty() {
        return Ok((lang, "select".to_string(), vec![], Some(vec![])));
    }

    // Fetch tags for all series in batch
    let series_tags = db
        .get_series_tags_batch(&series_ids)
        .await
        .unwrap_or_default();

    let metric_name = q
        .selector
        .measurement
        .clone()
        .unwrap_or_else(|| "unknown".to_string());

    // Query each series separately and build series data
    let mut series_data: Vec<SeriesData> = Vec::new();
    let mut all_points: Vec<DataPoint> = Vec::new();

    for series_id in &series_ids {
        match db.query(*series_id, q.time_range).await {
            Ok(mut points) => {
                points.sort_by_key(|p| p.timestamp);

                let tags = series_tags.get(series_id).cloned().unwrap_or_default();
                let mut tag_set: Vec<String> =
                    tags.iter().map(|(k, v)| format!("{}:{}", k, v)).collect();
                tag_set.sort();
                let scope = tag_set.join(",");

                let pointlist: Vec<[f64; 2]> = points
                    .iter()
                    .map(|p| [p.timestamp as f64, p.value])
                    .collect();
                let length = pointlist.len();

                series_data.push(SeriesData {
                    metric: metric_name.clone(),
                    scope,
                    tag_set,
                    pointlist,
                    length,
                });

                all_points.extend(points);
            },
            Err(e) => {
                tracing::warn!("Error querying series {}: {}", series_id, e);
            },
        }
    }

    all_points.sort_by_key(|p| p.timestamp);

    // Apply limit and offset
    let mut result_points = all_points;
    if let Some(offset) = q.offset {
        result_points = result_points.into_iter().skip(offset).collect();
    }
    if let Some(limit) = q.limit {
        result_points = result_points.into_iter().take(limit).collect();
    }

    Ok((lang, "select".to_string(), result_points, Some(series_data)))
}

/// Execute an AGGREGATE query with optional GROUP BY and time windowing support
async fn execute_aggregate(
    db: &TimeSeriesDB,
    q: AggregateQuery,
    lang: QueryLanguage,
) -> Result<
    (
        QueryLanguage,
        String,
        Vec<DataPoint>,
        AggregateExecutionResult,
    ),
    String,
> {
    let func_name = format!("{:?}", q.aggregation.function).to_lowercase();
    let mut warnings: Vec<String> = Vec::new();

    // Get series IDs using selector's tag filter
    let series_ids: Vec<SeriesId> = match q.selector.series_id {
        Some(id) => vec![id],
        None => {
            if let Some(ref measurement) = q.selector.measurement {
                let tag_filter = q.selector.to_tag_filter();
                db.find_series(measurement, &tag_filter)
                    .await
                    .map_err(|e| format!("Series lookup error: {}", e))?
            } else {
                return Err("No series_id or measurement specified".to_string());
            }
        },
    };

    if series_ids.is_empty() {
        return Ok((
            lang,
            "aggregate".to_string(),
            vec![],
            AggregateExecutionResult {
                func_name,
                value: f64::NAN,
                groups: None,
                time_aggregation: None,
                warnings: vec![],
                buckets: None,
            },
        ));
    }

    // Determine time window configuration
    let time_range_ms = q.time_range.end - q.time_range.start;
    let (use_time_window, interval_ms, interval_str, time_mode) =
        if let Some(ref window_spec) = q.aggregation.window {
            let interval_ms = window_spec.duration.as_millis() as i64;
            let interval_str = format_interval_ms(interval_ms);
            if let Some(warning) = validate_interval(interval_ms, time_range_ms) {
                warnings.push(warning);
            }
            (true, interval_ms, interval_str, "custom".to_string())
        } else {
            let (interval_ms, interval_str) =
                calculate_auto_interval(time_range_ms, DEFAULT_TARGET_POINTS);
            (true, interval_ms, interval_str, "auto".to_string())
        };

    let group_by_tags = &q.aggregation.group_by;
    let has_group_by = !group_by_tags.is_empty();

    if has_group_by {
        // GROUP BY execution path with optional time windowing
        let series_tags = db
            .index()
            .get_series_tags_batch(&series_ids)
            .await
            .map_err(|e| format!("Failed to fetch series tags: {}", e))?;

        // Group series by tag values
        let mut groups: HashMap<Vec<(String, String)>, Vec<SeriesId>> = HashMap::new();

        for series_id in &series_ids {
            let tags = series_tags.get(series_id).cloned().unwrap_or_default();
            let mut group_key: Vec<(String, String)> = group_by_tags
                .iter()
                .map(|tag_key| {
                    let value = tags.get(tag_key).cloned().unwrap_or_default();
                    (tag_key.clone(), value)
                })
                .collect();
            group_key.sort();
            groups.entry(group_key).or_default().push(*series_id);
        }

        // Compute aggregation for each group
        let mut grouped_results: Vec<GroupedAggregationResult> = Vec::new();
        let mut total_bucket_count = 0usize;

        for (group_key, group_series_ids) in groups {
            let mut group_points: Vec<DataPoint> = Vec::new();
            for series_id in &group_series_ids {
                match db.query(*series_id, q.time_range).await {
                    Ok(points) => group_points.extend(points),
                    Err(e) => tracing::warn!("Error querying series {}: {}", series_id, e),
                }
            }

            group_points.sort_by_key(|p| p.timestamp);
            let (_, value) = compute_aggregation_from_ast(&q.aggregation.function, &group_points);

            let buckets = if use_time_window && !group_points.is_empty() {
                let b = aggregate_into_buckets(
                    &group_points,
                    interval_ms,
                    q.time_range,
                    &q.aggregation.function,
                );
                total_bucket_count = total_bucket_count.max(b.len());
                Some(b)
            } else {
                None
            };

            let mut tag_set: Vec<String> = group_key
                .iter()
                .map(|(k, v)| format!("{}:{}", k, v))
                .collect();
            tag_set.sort();
            let scope = tag_set.join(",");

            grouped_results.push(GroupedAggregationResult {
                scope,
                tag_set,
                value,
                point_count: group_points.len(),
                buckets,
            });
        }

        grouped_results.sort_by(|a, b| a.scope.cmp(&b.scope));

        // Compute total aggregation
        let total_value = if grouped_results.is_empty() {
            f64::NAN
        } else {
            match q.aggregation.function {
                QueryAggFunction::Sum | QueryAggFunction::Count => {
                    grouped_results.iter().map(|g| g.value).sum()
                },
                _ => {
                    let mut all_points: Vec<DataPoint> = Vec::new();
                    for series_id in &series_ids {
                        if let Ok(points) = db.query(*series_id, q.time_range).await {
                            all_points.extend(points);
                        }
                    }
                    all_points.sort_by_key(|p| p.timestamp);
                    let (_, val) =
                        compute_aggregation_from_ast(&q.aggregation.function, &all_points);
                    val
                },
            }
        };

        let time_aggregation = if use_time_window {
            Some(create_time_aggregation_info(
                time_mode,
                interval_str,
                interval_ms,
                total_bucket_count,
            ))
        } else {
            None
        };

        Ok((
            lang,
            "aggregate".to_string(),
            vec![],
            AggregateExecutionResult {
                func_name,
                value: total_value,
                groups: Some(grouped_results),
                time_aggregation,
                warnings,
                buckets: None,
            },
        ))
    } else {
        // Simple aggregation (no GROUP BY) with optional time windowing
        let mut all_points: Vec<DataPoint> = Vec::new();
        for series_id in &series_ids {
            match db.query(*series_id, q.time_range).await {
                Ok(points) => all_points.extend(points),
                Err(e) => tracing::warn!("Error querying series {}: {}", series_id, e),
            }
        }

        if all_points.is_empty() {
            return Ok((
                lang,
                "aggregate".to_string(),
                vec![],
                AggregateExecutionResult {
                    func_name,
                    value: f64::NAN,
                    groups: None,
                    time_aggregation: None,
                    warnings: vec![],
                    buckets: None,
                },
            ));
        }

        all_points.sort_by_key(|p| p.timestamp);
        let (_, value) = compute_aggregation_from_ast(&q.aggregation.function, &all_points);

        let (buckets, time_aggregation) = if use_time_window {
            let b = aggregate_into_buckets(
                &all_points,
                interval_ms,
                q.time_range,
                &q.aggregation.function,
            );
            let bucket_count = b.len();
            (
                Some(b),
                Some(create_time_aggregation_info(
                    time_mode,
                    interval_str,
                    interval_ms,
                    bucket_count,
                )),
            )
        } else {
            (None, None)
        };

        Ok((
            lang,
            "aggregate".to_string(),
            vec![],
            AggregateExecutionResult {
                func_name,
                value,
                groups: None,
                time_aggregation,
                warnings,
                buckets,
            },
        ))
    }
}

/// Execute a DOWNSAMPLE query
async fn execute_downsample(
    db: &TimeSeriesDB,
    q: DownsampleQuery,
    lang: QueryLanguage,
) -> Result<(QueryLanguage, String, Vec<DataPoint>), String> {
    let series_ids: Vec<SeriesId> = match q.selector.series_id {
        Some(id) => vec![id],
        None => {
            if let Some(ref measurement) = q.selector.measurement {
                let tag_filter = q.selector.to_tag_filter();
                db.find_series(measurement, &tag_filter)
                    .await
                    .map_err(|e| format!("Series lookup error: {}", e))?
            } else {
                return Err("No series_id or measurement specified".to_string());
            }
        },
    };

    if series_ids.is_empty() {
        return Ok((lang, "downsample".to_string(), vec![]));
    }

    let mut all_points: Vec<DataPoint> = Vec::new();
    for series_id in &series_ids {
        match db.query(*series_id, q.time_range).await {
            Ok(points) => all_points.extend(points),
            Err(e) => tracing::warn!("Error querying series {}: {}", series_id, e),
        }
    }

    all_points.sort_by_key(|p| p.timestamp);

    // Apply proper downsampling algorithm based on method
    let downsampled = downsample_points(&all_points, q.target_points, q.method);

    Ok((lang, "downsample".to_string(), downsampled))
}

// =============================================================================
// Downsampling Algorithms
// =============================================================================

/// Apply downsampling algorithm to data points
///
/// Dispatches to the appropriate algorithm based on the method:
/// - LTTB: Preserves visual shape using triangle area optimization
/// - M4: Preserves min/max extremes in each bucket
/// - Average: Simple bucket averaging
/// - MinMax: Same as M4 (preserves extremes)
/// - First/Last: First or last point per bucket
fn downsample_points(
    points: &[DataPoint],
    target_points: usize,
    method: DownsampleMethod,
) -> Vec<DataPoint> {
    let n = points.len();

    // No downsampling needed if we have fewer points than target
    if n <= target_points {
        return points.to_vec();
    }

    // Need at least 3 points for LTTB to work properly
    let target = target_points.max(3);

    match method {
        DownsampleMethod::Lttb => lttb_downsample(points, target),
        DownsampleMethod::M4 | DownsampleMethod::MinMax => m4_downsample(points, target),
        DownsampleMethod::Average => average_downsample(points, target),
        DownsampleMethod::First => first_per_bucket(points, target),
        DownsampleMethod::Last => last_per_bucket(points, target),
    }
}

/// LTTB (Largest Triangle Three Buckets) algorithm
///
/// Selects points that maximize the triangle area with neighbors,
/// preserving the visual shape of the time series. This is the
/// recommended algorithm for charting/visualization use cases.
///
/// # Algorithm
/// 1. Always keep first and last points
/// 2. Divide remaining points into (target - 2) buckets
/// 3. For each bucket, select the point that forms the largest
///    triangle with the previous selected point and the average
///    of the next bucket
///
/// # Time complexity: O(n)
/// # Space complexity: O(target_points)
fn lttb_downsample(points: &[DataPoint], target: usize) -> Vec<DataPoint> {
    let n = points.len();

    // Edge case: too few points or small target
    if n <= 2 || target <= 2 {
        return if n > 0 {
            vec![points[0], points[n - 1]]
        } else {
            vec![]
        };
    }

    let mut result = Vec::with_capacity(target);

    // Always include first point
    result.push(points[0]);

    // Bucket size for middle points (excluding first and last)
    let bucket_size = (n - 2) as f64 / (target - 2) as f64;

    let mut prev_selected_idx = 0;

    for i in 0..(target - 2) {
        // Current bucket boundaries
        let bucket_start = ((i as f64) * bucket_size).floor() as usize + 1;
        let bucket_end = (((i + 1) as f64) * bucket_size).floor() as usize + 1;
        let bucket_end = bucket_end.min(n - 1);

        // Next bucket average point (for triangle calculation)
        let next_bucket_start = bucket_end;
        let next_bucket_end = (((i + 2) as f64) * bucket_size).floor() as usize + 1;
        let next_bucket_end = next_bucket_end.min(n);

        // Calculate average timestamp and value for next bucket
        let (avg_ts, avg_val) = if next_bucket_start < next_bucket_end {
            let count = (next_bucket_end - next_bucket_start) as f64;
            let sum_ts: i64 = points[next_bucket_start..next_bucket_end]
                .iter()
                .map(|p| p.timestamp)
                .sum();
            let sum_val: f64 = points[next_bucket_start..next_bucket_end]
                .iter()
                .map(|p| p.value)
                .sum();
            (sum_ts as f64 / count, sum_val / count)
        } else {
            (points[n - 1].timestamp as f64, points[n - 1].value)
        };

        // Find point in current bucket with largest triangle area
        let prev = &points[prev_selected_idx];
        let mut max_area = -1.0f64;
        let mut max_idx = bucket_start;

        for (j, curr) in points
            .iter()
            .enumerate()
            .take(bucket_end)
            .skip(bucket_start)
        {
            // Triangle area using cross product formula:
            // Area = 0.5 * |x1(y2-y3) + x2(y3-y1) + x3(y1-y2)|
            let area = ((prev.timestamp as f64 - avg_ts) * (curr.value - prev.value)
                - (prev.timestamp as f64 - curr.timestamp as f64) * (avg_val - prev.value))
                .abs()
                * 0.5;

            if area > max_area {
                max_area = area;
                max_idx = j;
            }
        }

        result.push(points[max_idx]);
        prev_selected_idx = max_idx;
    }

    // Always include last point
    result.push(points[n - 1]);

    result
}

/// M4 algorithm - preserves min/max/first/last per bucket
///
/// Each bucket contributes up to 4 points (min, max, first, last),
/// ensuring peaks and valleys are never hidden. This is ideal for
/// displaying data where extremes are important (e.g., error rates,
/// latency spikes).
///
/// # Time complexity: O(n)
/// # Space complexity: O(target_points)
fn m4_downsample(points: &[DataPoint], target: usize) -> Vec<DataPoint> {
    let n = points.len();

    if n == 0 {
        return vec![];
    }

    // M4 produces up to 4 points per bucket, so we need target/4 buckets
    let num_buckets = (target / 4).max(1);
    let bucket_size = (n as f64 / num_buckets as f64).ceil() as usize;

    let mut result = Vec::with_capacity(target);

    for bucket in 0..num_buckets {
        let start = bucket * bucket_size;
        let end = ((bucket + 1) * bucket_size).min(n);

        if start >= end {
            continue;
        }

        let bucket_points = &points[start..end];

        // Find min, max, first, last within bucket
        let first = bucket_points[0];
        let last = bucket_points[bucket_points.len() - 1];

        let mut min_point = first;
        let mut max_point = first;

        for p in bucket_points.iter() {
            if p.value < min_point.value {
                min_point = *p;
            }
            if p.value > max_point.value {
                max_point = *p;
            }
        }

        // Add points in timestamp order to maintain time sequence
        let mut bucket_result = vec![first, min_point, max_point, last];
        bucket_result.sort_by_key(|p| p.timestamp);
        bucket_result.dedup_by_key(|p| p.timestamp); // Remove duplicates

        result.extend(bucket_result);
    }

    // Trim to target if we produced too many points
    result.truncate(target);

    result
}

/// Simple average per bucket downsampling
///
/// Divides data into equal-sized buckets and outputs one point per bucket
/// with the average timestamp and value. Produces smooth output but may
/// hide peaks and valleys.
fn average_downsample(points: &[DataPoint], target: usize) -> Vec<DataPoint> {
    let n = points.len();
    let bucket_size = (n as f64 / target as f64).ceil() as usize;

    let mut result = Vec::with_capacity(target);

    for bucket in 0..target {
        let start = bucket * bucket_size;
        let end = ((bucket + 1) * bucket_size).min(n);

        if start >= end {
            break;
        }

        let bucket_points = &points[start..end];

        // Compute average timestamp and value
        let avg_ts =
            bucket_points.iter().map(|p| p.timestamp).sum::<i64>() / bucket_points.len() as i64;
        let avg_val =
            bucket_points.iter().map(|p| p.value).sum::<f64>() / bucket_points.len() as f64;

        // Use series_id from first point in bucket
        result.push(DataPoint::new(bucket_points[0].series_id, avg_ts, avg_val));
    }

    result
}

/// First point per bucket downsampling
///
/// Selects the first point from each bucket. Simple and fast,
/// but may miss important data at bucket boundaries.
fn first_per_bucket(points: &[DataPoint], target: usize) -> Vec<DataPoint> {
    let n = points.len();
    let bucket_size = (n as f64 / target as f64).ceil() as usize;

    (0..target)
        .filter_map(|bucket| {
            let idx = bucket * bucket_size;
            if idx < n {
                Some(points[idx])
            } else {
                None
            }
        })
        .collect()
}

/// Last point per bucket downsampling
///
/// Selects the last point from each bucket. Useful when you care
/// more about the final state at each interval.
fn last_per_bucket(points: &[DataPoint], target: usize) -> Vec<DataPoint> {
    let n = points.len();
    let bucket_size = (n as f64 / target as f64).ceil() as usize;

    (0..target)
        .filter_map(|bucket| {
            let idx = ((bucket + 1) * bucket_size - 1).min(n - 1);
            if bucket * bucket_size < n {
                Some(points[idx])
            } else {
                None
            }
        })
        .collect()
}

/// Execute a LATEST query using optimized reverse scan (ENH-001)
///
/// This function uses the optimized `query_latest` method which scans chunks
/// in reverse chronological order (newest first) and stops early once enough
/// points are found. This provides O(1) performance for typical LATEST queries
/// instead of O(n) when scanning all data.
///
/// # Performance
///
/// - **Single series, few points:** ~10-50Âµs (reads only most recent chunk)
/// - **Multiple series:** Parallelized using the optimized method per series
/// - **Large series:** Early termination prevents unnecessary chunk reads
async fn execute_latest(
    db: &TimeSeriesDB,
    q: LatestQuery,
    lang: QueryLanguage,
) -> Result<(QueryLanguage, String, Vec<DataPoint>), String> {
    let series_ids: Vec<SeriesId> = match q.selector.series_id {
        Some(id) => vec![id],
        None => {
            if let Some(ref measurement) = q.selector.measurement {
                let tag_filter = q.selector.to_tag_filter();
                db.find_series(measurement, &tag_filter)
                    .await
                    .map_err(|e| format!("Series lookup error: {}", e))?
            } else {
                return Err("No series_id or measurement specified".to_string());
            }
        },
    };

    if series_ids.is_empty() {
        return Ok((lang, "latest".to_string(), vec![]));
    }

    // ENH-001: Use optimized query_latest for each series
    // This scans chunks from newest to oldest and stops early
    let mut all_points: Vec<DataPoint> = Vec::new();
    for series_id in &series_ids {
        match db.query_latest(*series_id, q.count).await {
            Ok(points) => all_points.extend(points),
            Err(e) => tracing::warn!("Error querying series {}: {}", series_id, e),
        }
    }

    // For multiple series, we need to merge and take the latest across all
    if series_ids.len() > 1 {
        all_points.sort_by_key(|p| p.timestamp);
        let latest: Vec<DataPoint> = all_points
            .into_iter()
            .rev()
            .take(q.count)
            .collect::<Vec<_>>()
            .into_iter()
            .rev()
            .collect();
        Ok((lang, "latest".to_string(), latest))
    } else {
        // Single series - already optimally sorted from query_latest
        Ok((lang, "latest".to_string(), all_points))
    }
}

// =============================================================================
// EXPLAIN Plan Generation
// =============================================================================

/// Generate comprehensive EXPLAIN plan for a query
///
/// Creates a detailed execution plan with:
/// - Human-readable description of query behavior
/// - Step-by-step execution details
/// - Cost estimates
/// - Applied optimizations
fn generate_explain_plan(query: &ParsedQuery) -> ExplainResult {
    // Generate human-readable description
    let description = generate_query_description(query);

    // Generate step-by-step execution plan
    let execution_steps = generate_execution_steps(query);

    // Determine query type
    let query_type = match query {
        ParsedQuery::Select(_) => "select",
        ParsedQuery::Aggregate(_) => "aggregate",
        ParsedQuery::Downsample(_) => "downsample",
        ParsedQuery::Latest(_) => "latest",
        ParsedQuery::Explain(inner) => return generate_explain_plan(inner),
        ParsedQuery::Stream(_) => "stream",
    }
    .to_string();

    // Generate logical plan representation
    let logical_plan = generate_logical_plan_string(query);

    // Estimate costs (heuristic-based since we don't have statistics)
    let cost_estimate = estimate_query_cost(query);

    // Standard optimizations applied by the query engine
    let optimizations = vec![
        OptimizationInfo {
            name: "predicate_pushdown".to_string(),
            applied: true,
            description:
                "Pushes filter predicates into the scan operator to reduce data read from storage"
                    .to_string(),
        },
        OptimizationInfo {
            name: "projection_pushdown".to_string(),
            applied: true,
            description: "Only reads required columns from storage, reducing I/O".to_string(),
        },
        OptimizationInfo {
            name: "time_range_pruning".to_string(),
            applied: true,
            description: "Uses chunk metadata to skip chunks outside the query time range"
                .to_string(),
        },
    ];

    ExplainResult {
        description,
        execution_steps,
        logical_plan,
        cost_estimate,
        query_type,
        optimizations,
    }
}

/// Generate human-readable description of query behavior
fn generate_query_description(query: &ParsedQuery) -> String {
    match query {
        ParsedQuery::Select(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("unknown");
            let tag_info = if q.selector.tag_filters.is_empty() {
                "all series".to_string()
            } else {
                format!("{} tag filter(s)", q.selector.tag_filters.len())
            };
            let limit_info = q
                .limit
                .map(|l| format!(", limited to {} points", l))
                .unwrap_or_default();

            format!(
                "SELECT query that retrieves raw data points from metric '{}' \
                 matching {} within the specified time range{}. \
                 Points are sorted by timestamp in ascending order.",
                metric, tag_info, limit_info
            )
        },
        ParsedQuery::Aggregate(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("unknown");
            let func_name = format!("{:?}", q.aggregation.function).to_lowercase();
            let window_info = q
                .aggregation
                .window
                .as_ref()
                .map(|w| format!(" with {}ms time windows", w.duration.as_millis()))
                .unwrap_or_default();
            let group_info = if q.aggregation.group_by.is_empty() {
                String::new()
            } else {
                format!(", grouped by tags: {:?}", q.aggregation.group_by)
            };

            format!(
                "AGGREGATE query that computes {} over metric '{}'{}{}.  \
                 Execution: 1) Find matching series via Redis SINTER on tag indexes, \
                 2) Read data points from storage chunks, \
                 3) Apply {} aggregation function to values{}.",
                func_name,
                metric,
                window_info,
                group_info,
                func_name,
                if q.aggregation.window.is_some() {
                    " per time bucket"
                } else {
                    ""
                }
            )
        },
        ParsedQuery::Downsample(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("unknown");
            let method = format!("{:?}", q.method);

            format!(
                "DOWNSAMPLE query that reduces data points from metric '{}' to {} points \
                 using the {} algorithm. \
                 LTTB preserves visual shape by selecting points that maximize triangle area. \
                 M4 preserves min/max extremes in each bucket. \
                 Useful for rendering charts with limited pixel resolution.",
                metric, q.target_points, method
            )
        },
        ParsedQuery::Latest(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("unknown");

            format!(
                "LATEST query that retrieves the {} most recent data point(s) from metric '{}'. \
                 Uses optimized reverse-time scan that stops after finding required points, \
                 avoiding full time range scan.",
                q.count, metric
            )
        },
        ParsedQuery::Stream(_) => {
            "STREAM query that creates a continuous subscription to new data. \
             Not executable via HTTP - requires WebSocket connection."
                .to_string()
        },
        ParsedQuery::Explain(inner) => {
            format!(
                "EXPLAIN query that shows execution plan for: {}",
                generate_query_description(inner)
            )
        },
    }
}

/// Generate step-by-step execution details
fn generate_execution_steps(query: &ParsedQuery) -> Vec<ExecutionStep> {
    match query {
        ParsedQuery::Select(q) => {
            let mut steps = vec![];
            let metric = q.selector.measurement.as_deref().unwrap_or("metric");

            steps.push(ExecutionStep {
                step: 1,
                operation: "SeriesLookup".to_string(),
                description: format!(
                    "Find series IDs matching metric '{}' and tag filters using Redis SINTER",
                    metric
                ),
                input: Some("Tag filters".to_string()),
                output: "Vec<SeriesId>".to_string(),
            });

            steps.push(ExecutionStep {
                step: 2,
                operation: "ChunkScan".to_string(),
                description: "For each series, scan storage chunks within time range. \
                             Uses chunk time boundaries to skip irrelevant chunks."
                    .to_string(),
                input: Some("SeriesId, TimeRange".to_string()),
                output: "Iterator<DataPoint>".to_string(),
            });

            if !q.predicates.is_empty() {
                steps.push(ExecutionStep {
                    step: 3,
                    operation: "Filter".to_string(),
                    description: format!(
                        "Apply {} value predicate(s) to filter points",
                        q.predicates.len()
                    ),
                    input: Some("DataPoints".to_string()),
                    output: "Filtered DataPoints".to_string(),
                });
            }

            steps.push(ExecutionStep {
                step: steps.len() as u32 + 1,
                operation: "Sort".to_string(),
                description: "Sort points by timestamp ascending".to_string(),
                input: Some("DataPoints".to_string()),
                output: "Sorted DataPoints".to_string(),
            });

            if let Some(limit) = q.limit {
                steps.push(ExecutionStep {
                    step: steps.len() as u32 + 1,
                    operation: "Limit".to_string(),
                    description: format!("Truncate to first {} points", limit),
                    input: Some("Sorted DataPoints".to_string()),
                    output: "Limited DataPoints".to_string(),
                });
            }

            steps
        },
        ParsedQuery::Aggregate(q) => {
            let mut steps = vec![];
            let metric = q.selector.measurement.as_deref().unwrap_or("metric");
            let func = format!("{:?}", q.aggregation.function);

            steps.push(ExecutionStep {
                step: 1,
                operation: "SeriesLookup".to_string(),
                description: format!("Find series IDs for metric '{}' via Redis index", metric),
                input: Some("Metric name, Tag filters".to_string()),
                output: "Vec<SeriesId>".to_string(),
            });

            if !q.aggregation.group_by.is_empty() {
                steps.push(ExecutionStep {
                    step: 2,
                    operation: "TagFetch".to_string(),
                    description: format!(
                        "Fetch tag values for group_by keys {:?} from Redis",
                        q.aggregation.group_by
                    ),
                    input: Some("SeriesIds".to_string()),
                    output: "HashMap<SeriesId, Tags>".to_string(),
                });

                steps.push(ExecutionStep {
                    step: 3,
                    operation: "GroupSeries".to_string(),
                    description: format!(
                        "Group series by tag combination {:?}",
                        q.aggregation.group_by
                    ),
                    input: Some("SeriesIds, Tags".to_string()),
                    output: "HashMap<TagCombination, Vec<SeriesId>>".to_string(),
                });
            }

            steps.push(ExecutionStep {
                step: steps.len() as u32 + 1,
                operation: "ChunkScan".to_string(),
                description: "Read data points from storage for each series".to_string(),
                input: Some("SeriesIds, TimeRange".to_string()),
                output: "Vec<DataPoint>".to_string(),
            });

            if q.aggregation.window.is_some() {
                steps.push(ExecutionStep {
                    step: steps.len() as u32 + 1,
                    operation: "TimeBucket".to_string(),
                    description: format!(
                        "Group points into time buckets of {}ms",
                        q.aggregation
                            .window
                            .as_ref()
                            .map(|w| w.duration.as_millis())
                            .unwrap_or(0)
                    ),
                    input: Some("DataPoints".to_string()),
                    output: "HashMap<BucketTimestamp, Vec<f64>>".to_string(),
                });
            }

            steps.push(ExecutionStep {
                step: steps.len() as u32 + 1,
                operation: "Aggregate".to_string(),
                description: format!(
                    "Apply {} function to values{}",
                    func,
                    if q.aggregation.window.is_some() {
                        " per bucket"
                    } else {
                        ""
                    }
                ),
                input: Some("Values".to_string()),
                output: if q.aggregation.window.is_some() {
                    "Vec<(timestamp, aggregated_value)>".to_string()
                } else {
                    "Single aggregated value".to_string()
                },
            });

            steps
        },
        ParsedQuery::Downsample(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("metric");
            let method = format!("{:?}", q.method);

            vec![
                ExecutionStep {
                    step: 1,
                    operation: "SeriesLookup".to_string(),
                    description: format!("Find series IDs for metric '{}'", metric),
                    input: Some("Metric name".to_string()),
                    output: "Vec<SeriesId>".to_string(),
                },
                ExecutionStep {
                    step: 2,
                    operation: "ChunkScan".to_string(),
                    description: "Read all data points from storage".to_string(),
                    input: Some("SeriesIds, TimeRange".to_string()),
                    output: "Vec<DataPoint>".to_string(),
                },
                ExecutionStep {
                    step: 3,
                    operation: "Sort".to_string(),
                    description: "Sort points by timestamp".to_string(),
                    input: Some("DataPoints".to_string()),
                    output: "Sorted DataPoints".to_string(),
                },
                ExecutionStep {
                    step: 4,
                    operation: format!("Downsample::{}", method),
                    description: format!(
                        "Apply {} algorithm to reduce to {} points. \
                         Algorithm complexity: O(n), produces target points that \
                         best represent the visual shape of the data.",
                        method, q.target_points
                    ),
                    input: Some("Sorted DataPoints".to_string()),
                    output: format!("Vec<DataPoint> (max {} points)", q.target_points),
                },
            ]
        },
        ParsedQuery::Latest(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("metric");

            vec![
                ExecutionStep {
                    step: 1,
                    operation: "SeriesLookup".to_string(),
                    description: format!("Find series IDs for metric '{}'", metric),
                    input: Some("Metric name".to_string()),
                    output: "Vec<SeriesId>".to_string(),
                },
                ExecutionStep {
                    step: 2,
                    operation: "ReverseChunkScan".to_string(),
                    description: format!(
                        "Scan chunks in reverse time order, stop after finding {} point(s). \
                         Optimized to avoid reading entire time range.",
                        q.count
                    ),
                    input: Some("SeriesIds".to_string()),
                    output: format!("Vec<DataPoint> (max {} points)", q.count),
                },
            ]
        },
        ParsedQuery::Explain(inner) => generate_execution_steps(inner),
        ParsedQuery::Stream(_) => vec![ExecutionStep {
            step: 1,
            operation: "WebSocketSubscription".to_string(),
            description: "Creates a streaming subscription. Not available via HTTP.".to_string(),
            input: None,
            output: "Stream<DataPoint>".to_string(),
        }],
    }
}

/// Generate logical plan string representation
fn generate_logical_plan_string(query: &ParsedQuery) -> String {
    match query {
        ParsedQuery::Select(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("*");
            let time_range = format!("{}..{}", q.time_range.start, q.time_range.end);
            let mut plan = format!("Scan(selector={}, time_range={})", metric, time_range);

            if !q.predicates.is_empty() {
                plan = format!("Filter(predicates={})\n  {}", q.predicates.len(), plan);
            }

            if q.limit.is_some() || q.offset.is_some() {
                plan = format!(
                    "Limit(limit={:?}, offset={:?})\n  Sort(timestamp ASC)\n    {}",
                    q.limit, q.offset, plan
                );
            } else {
                plan = format!("Sort(timestamp ASC)\n  {}", plan);
            }

            plan
        },
        ParsedQuery::Aggregate(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("*");
            let func = format!("{:?}", q.aggregation.function);
            let time_range = format!("{}..{}", q.time_range.start, q.time_range.end);

            let scan = format!("Scan(selector={}, time_range={})", metric, time_range);

            let window_info = q
                .aggregation
                .window
                .as_ref()
                .map(|w| format!(", window={}ms", w.duration.as_millis()))
                .unwrap_or_default();

            let group_info = if q.aggregation.group_by.is_empty() {
                String::new()
            } else {
                format!(", group_by={:?}", q.aggregation.group_by)
            };

            format!(
                "Aggregate(function={}{}{})\n  {}",
                func, window_info, group_info, scan
            )
        },
        ParsedQuery::Downsample(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("*");
            let method = format!("{:?}", q.method);
            let time_range = format!("{}..{}", q.time_range.start, q.time_range.end);

            format!(
                "Downsample(method={}, target={})\n  Sort(timestamp ASC)\n    Scan(selector={}, time_range={})",
                method, q.target_points, metric, time_range
            )
        },
        ParsedQuery::Latest(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("*");

            format!(
                "Latest(count={})\n  ReverseScan(selector={}, from=NOW)",
                q.count, metric
            )
        },
        ParsedQuery::Explain(inner) => generate_logical_plan_string(inner),
        ParsedQuery::Stream(q) => {
            let metric = q.selector.measurement.as_deref().unwrap_or("*");
            format!("Stream(selector={})", metric)
        },
    }
}

/// Estimate query cost (heuristic-based)
fn estimate_query_cost(query: &ParsedQuery) -> CostEstimateInfo {
    // Default estimates for when we don't have statistics
    let (rows, bytes, chunks) = match query {
        ParsedQuery::Select(q) => {
            let time_span_ms = q.time_range.end - q.time_range.start;
            // Estimate: 1 point per second per series, 16 bytes per point
            let estimated_points = (time_span_ms / 1000).max(1) as u64;
            let estimated_bytes = estimated_points * 16;
            // Chunks are typically 2 hours each
            let chunks = ((time_span_ms as f64) / 7_200_000.0).ceil() as u64;
            (estimated_points.min(10000), estimated_bytes, chunks.max(1))
        },
        ParsedQuery::Aggregate(q) => {
            let time_span_ms = q.time_range.end - q.time_range.start;
            let estimated_points = (time_span_ms / 1000).max(1) as u64;
            let chunks = ((time_span_ms as f64) / 7_200_000.0).ceil() as u64;
            // Aggregation reads all points but returns fewer
            (
                estimated_points.min(50000),
                estimated_points * 16,
                chunks.max(1),
            )
        },
        ParsedQuery::Downsample(q) => {
            let time_span_ms = q.time_range.end - q.time_range.start;
            let estimated_points = (time_span_ms / 1000).max(1) as u64;
            let chunks = ((time_span_ms as f64) / 7_200_000.0).ceil() as u64;
            // Downsampling reads all but returns target_points
            (
                estimated_points.min(100000),
                estimated_points * 16,
                chunks.max(1),
            )
        },
        ParsedQuery::Latest(_) => {
            // Latest is optimized - scans from most recent chunk
            (10, 160, 1)
        },
        ParsedQuery::Explain(inner) => return estimate_query_cost(inner),
        ParsedQuery::Stream(_) => (0, 0, 0),
    };

    let execution_category = categorize_execution_cost(rows, chunks);

    CostEstimateInfo {
        estimated_rows: rows,
        estimated_bytes: bytes,
        chunks_to_scan: chunks,
        execution_category,
    }
}

/// Categorize execution cost for user-friendly display
fn categorize_execution_cost(rows: u64, chunks: u64) -> String {
    if rows < 1000 && chunks < 5 {
        "fast (<10ms)".to_string()
    } else if rows < 100_000 && chunks < 50 {
        "medium (10-100ms)".to_string()
    } else if rows < 1_000_000 && chunks < 500 {
        "slow (100ms-1s)".to_string()
    } else {
        "very_slow (>1s)".to_string()
    }
}
